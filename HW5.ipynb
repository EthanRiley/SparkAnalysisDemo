{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2786f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/10 20:55:59 WARN Utils: Your hostname, Ritheshs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.0.0.235 instead (on interface en0)\n",
      "23/04/10 20:55:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/10 20:56:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, sqrt\n",
    "from pyspark.sql.types import DoubleType\n",
    "import math\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Temperature Analysis\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "162c3555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+--------+\n",
      "|   _c0| _c1|    _c2|     _c3|\n",
      "+------+----+-------+--------+\n",
      "|014190|null|+59.050|+006.117|\n",
      "|020840|null|+68.050|+022.000|\n",
      "|021670|null|+55.383|+012.817|\n",
      "|022690|null|+63.183|+019.017|\n",
      "|027880|null|+63.150|+027.317|\n",
      "|029740|null|+60.317|+024.963|\n",
      "|033020|null|+53.248|-004.535|\n",
      "|041400|null|+63.850|-021.383|\n",
      "|041950|null|+65.280|-014.000|\n",
      "|062670|null|+52.900|+005.383|\n",
      "|071540|null|+48.800|+002.500|\n",
      "|072660|null|+47.850|+003.497|\n",
      "|080550|null|+42.589|-005.656|\n",
      "|082330|null|+40.933|-001.300|\n",
      "|085440|null|+40.917|-008.633|\n",
      "|106450|null|+49.900|+009.433|\n",
      "|112980|null|+46.933|+015.867|\n",
      "|128430|null|+47.433|+019.183|\n",
      "|131680|null|+45.333|+019.850|\n",
      "|153350|null|+45.062|+028.714|\n",
      "+------+----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read a CSV file into a DataFrame\n",
    "df_stations = spark.read.csv(\"stations.csv\")\n",
    "\n",
    "df_stations = df_stations.dropDuplicates()\n",
    "df_stations = df_stations.dropna(subset=[\"_c2\", \"_c3\"], how=\"all\")\n",
    "\n",
    "df_stations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e2d262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28129"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stations.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eadbaa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+\n",
      "|   _c0| _c1|_c2|_c3| _c4|\n",
      "+------+----+---+---+----+\n",
      "|010010|null| 03| 10|29.4|\n",
      "|010010|null| 04| 05|34.9|\n",
      "|010080|null| 01| 11| 0.1|\n",
      "|010080|null| 09| 03|41.3|\n",
      "|010080|null| 09| 05|41.9|\n",
      "|010080|null| 09| 25|20.5|\n",
      "|010100|null| 08| 04|48.4|\n",
      "|010100|null| 08| 25|48.4|\n",
      "|010100|null| 09| 16|42.0|\n",
      "|010230|null| 01| 06|18.7|\n",
      "|010230|null| 05| 26|52.3|\n",
      "|010250|null| 05| 04|41.6|\n",
      "|010250|null| 07| 15|54.9|\n",
      "|010330|null| 06| 22|44.6|\n",
      "|010350|null| 06| 02|50.0|\n",
      "|010350|null| 07| 16|55.2|\n",
      "|010520|null| 03| 16|41.2|\n",
      "|010520|null| 05| 21|46.3|\n",
      "|010530|null| 08| 02|67.8|\n",
      "|010530|null| 08| 28|43.2|\n",
      "+------+----+---+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_1986 = spark.read.csv(\"1986.csv\")\n",
    "df_1986 = df_1986.dropDuplicates()\n",
    "\n",
    "df_1986 = df_1986.dropDuplicates()\n",
    "df_1986 = df_1986.dropna(subset=[\"_c2\", \"_c3\"], how=\"all\")\n",
    "\n",
    "df_1986.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25e92e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----+----+\n",
      "|   _c0| _c1|_c2|_c3| _c4| _c2| _c3|\n",
      "+------+----+---+---+----+----+----+\n",
      "|010010|null| 03| 10|29.4|null|null|\n",
      "|010010|null| 04| 05|34.9|null|null|\n",
      "|010080|null| 01| 11| 0.1|null|null|\n",
      "|010080|null| 09| 03|41.3|null|null|\n",
      "|010080|null| 09| 05|41.9|null|null|\n",
      "|010080|null| 09| 25|20.5|null|null|\n",
      "|010100|null| 08| 04|48.4|null|null|\n",
      "|010100|null| 08| 25|48.4|null|null|\n",
      "|010100|null| 09| 16|42.0|null|null|\n",
      "|010230|null| 01| 06|18.7|null|null|\n",
      "|010230|null| 05| 26|52.3|null|null|\n",
      "|010250|null| 05| 04|41.6|null|null|\n",
      "|010250|null| 07| 15|54.9|null|null|\n",
      "|010330|null| 06| 22|44.6|null|null|\n",
      "|010350|null| 06| 02|50.0|null|null|\n",
      "|010350|null| 07| 16|55.2|null|null|\n",
      "|010520|null| 03| 16|41.2|null|null|\n",
      "|010520|null| 05| 21|46.3|null|null|\n",
      "|010530|null| 08| 02|67.8|null|null|\n",
      "|010530|null| 08| 28|43.2|null|null|\n",
      "+------+----+---+---+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temperatures_df = df_1986.join(df_stations, [\"_c0\", \"_c1\"], \"left_outer\")\n",
    "temperatures_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6963daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    R = 6371  # Earth's radius in km\n",
    "    \n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    \n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    return R * c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65392de7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'DATE' does not exist. Did you mean one of the following? [_c0, _c1, _c2, _c2, _c3, _c3, _c4];\n'Filter ('DATE = 1986-01-28)\n+- Project [_c0#321, _c1#322, _c2#323, _c3#324, _c4#325, _c2#246, _c3#247]\n   +- Join LeftOuter, ((_c0#321 = _c0#244) AND (_c1#322 = _c1#245))\n      :- Filter atleastnnonnulls(1, _c2#323, _c3#324)\n      :  +- Deduplicate [_c3#324, _c0#321, _c4#325, _c1#322, _c2#323]\n      :     +- Deduplicate [_c3#324, _c0#321, _c4#325, _c1#322, _c2#323]\n      :        +- Relation [_c0#321,_c1#322,_c2#323,_c3#324,_c4#325] csv\n      +- Filter atleastnnonnulls(1, _c2#246, _c3#247)\n         +- Deduplicate [_c0#244, _c1#245, _c2#246, _c3#247]\n            +- Relation [_c0#244,_c1#245,_c2#246,_c3#247] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/44/2t5xy4657j7gb2np_jywbllh0000gn/T/ipykernel_63881/2084326396.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_stations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_stations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_c2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_c3\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtemperatures_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemperatures_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DATE\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1986-01-28\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   2077\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"condition should be string or Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column 'DATE' does not exist. Did you mean one of the following? [_c0, _c1, _c2, _c2, _c3, _c3, _c4];\n'Filter ('DATE = 1986-01-28)\n+- Project [_c0#321, _c1#322, _c2#323, _c3#324, _c4#325, _c2#246, _c3#247]\n   +- Join LeftOuter, ((_c0#321 = _c0#244) AND (_c1#322 = _c1#245))\n      :- Filter atleastnnonnulls(1, _c2#323, _c3#324)\n      :  +- Deduplicate [_c3#324, _c0#321, _c4#325, _c1#322, _c2#323]\n      :     +- Deduplicate [_c3#324, _c0#321, _c4#325, _c1#322, _c2#323]\n      :        +- Relation [_c0#321,_c1#322,_c2#323,_c3#324,_c4#325] csv\n      +- Filter atleastnnonnulls(1, _c2#246, _c3#247)\n         +- Deduplicate [_c0#244, _c1#245, _c2#246, _c3#247]\n            +- Relation [_c0#244,_c1#245,_c2#246,_c3#247] csv\n"
     ]
    }
   ],
   "source": [
    "df_stations = df_stations.dropna(subset=[\"_c2\", \"_c3\"])\n",
    "temperatures_df = temperatures_df.filter((col(\"DATE\") == \"1986-01-28\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cape_canaveral_coordinates = (28.3922, -80.6077)\n",
    "\n",
    "nearby_stations = stations_df.withColumn(\"distance\", haversine_udf(col(\"LATITUDE\"), col(\"LONGITUDE\"), lit(cape_canaveral_coordinates[0]), lit(cape_canaveral_coordinates[1]))).filter(col(\"distance\") <= 100)\n",
    "\n",
    "temp_nearby_stations = temperatures_df.join(nearby_stations, \"STATION\")\n",
    "temp_nearby_stations = temp_nearby_stations.withColumn(\"weight\", 1 / col(\"distance\"))\n",
    "weighted_temperature_sum = temp_nearby_stations.agg({\"weight\": \"sum\"}).collect()[0][0]\n",
    "temp_nearby_stations = temp_nearby_stations.withColumn(\"weighted_temperature\", col(\"TEMPERATURE\") * col(\"weight\"))\n",
    "weighted_temperature_at_cape = temp_nearby_stations.agg({\"weighted_temperature\": \"sum\"}).collect()[0][0] / weighted_temperature_sum\n",
    "\n",
    "print(f\"Estimated temperature at Cape Canaveral on January 28, 1986: {weighted_temperature_at_cape:.2f}Â°C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12313d",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "Rubric \n",
    "\n",
    " \n",
    "\n",
    "Clarity & Critical Thinking  \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "Reasoning & Support  \n",
    "\n",
    "\n",
    "\n",
    "Creativity & Risk-Taking  \n",
    "\n",
    "\n",
    "\n",
    "Organization & Coherence  \n",
    "\n",
    "\n",
    "\n",
    "Use of Sources/Citation  \n",
    "\n",
    "\n",
    "\n",
    "Tone/Style/Grammar  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
